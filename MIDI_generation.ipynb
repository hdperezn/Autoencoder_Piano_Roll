{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["vcptiUc_1jtG","KMQP3j1ak7Zn","vIxDNt0qPS4W","P1DKeid6aulr","HhoVzuYszHuJ","bbDO_GisoNfM","aB8uvPVxQ_if","Kix-3z1Fv5-e","IswYGiwQwQOX"],"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6426948,"sourceType":"datasetVersion","datasetId":3708051},{"sourceId":7420888,"sourceType":"datasetVersion","datasetId":4317501},{"sourceId":7433309,"sourceType":"datasetVersion","datasetId":4325767},{"sourceId":10170850,"sourceType":"datasetVersion","datasetId":6281493}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hmelmoth/midi-generation?scriptVersionId=217926923\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"vcptiUc_1jtG"}},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"executionInfo":{"elapsed":3872,"status":"ok","timestamp":1697987705316,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"},"user_tz":-180},"id":"R_rqQ3XsySl0","outputId":"5f9b67dd-0e23-4e2b-b072-389f837604cb","execution":{"iopub.status.busy":"2025-01-16T14:25:55.757804Z","iopub.execute_input":"2025-01-16T14:25:55.75824Z","iopub.status.idle":"2025-01-16T14:25:55.786448Z","shell.execute_reply.started":"2025-01-16T14:25:55.758206Z","shell.execute_reply":"2025-01-16T14:25:55.785125Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git\nfrom gcpds.image_segmentation.losses import DiceCoefficient","metadata":{"executionInfo":{"elapsed":20020,"status":"ok","timestamp":1697987725334,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"},"user_tz":-180},"id":"eIU901d-1m9D","outputId":"4f2a6de3-1c42-49d5-c4d3-7c450a1fc27d","execution":{"iopub.status.busy":"2025-01-16T14:25:55.788914Z","iopub.execute_input":"2025-01-16T14:25:55.789474Z","iopub.status.idle":"2025-01-16T14:26:25.353819Z","shell.execute_reply.started":"2025-01-16T14:25:55.789433Z","shell.execute_reply":"2025-01-16T14:26:25.352492Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git\n  Cloning https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git to /tmp/pip-req-build-q7lx6zug\n  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git /tmp/pip-req-build-q7lx6zug\n  Resolved https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git to commit 01b06059d3a0d2980eea4d0b1e964718cab173ee\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from gcpds-image-segmentation==0.1a0) (0.21.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from gcpds-image-segmentation==0.1a0) (3.7.4)\nCollecting gdown (from gcpds-image-segmentation==0.1a0)\n  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from gcpds-image-segmentation==0.1a0) (4.9.0.80)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from gcpds-image-segmentation==0.1a0) (1.2.2)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->gcpds-image-segmentation==0.1a0) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->gcpds-image-segmentation==0.1a0) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->gcpds-image-segmentation==0.1a0) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown->gcpds-image-segmentation==0.1a0) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (1.4.4)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->gcpds-image-segmentation==0.1a0) (2.8.2)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (3.1)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (2023.8.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (1.4.1)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image->gcpds-image-segmentation==0.1a0) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gcpds-image-segmentation==0.1a0) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gcpds-image-segmentation==0.1a0) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->gcpds-image-segmentation==0.1a0) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->gcpds-image-segmentation==0.1a0) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->gcpds-image-segmentation==0.1a0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->gcpds-image-segmentation==0.1a0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->gcpds-image-segmentation==0.1a0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->gcpds-image-segmentation==0.1a0) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->gcpds-image-segmentation==0.1a0) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: gcpds-image-segmentation\n  Building wheel for gcpds-image-segmentation (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gcpds-image-segmentation: filename=gcpds_image_segmentation-0.1a0-py3-none-any.whl size=39399 sha256=d7420e395ee6ee89b47addcffb068f86ecdc16769e7a24aeafa5e8ab24c48281\n  Stored in directory: /tmp/pip-ephem-wheel-cache-87xoz20s/wheels/88/5f/53/179661a69fb02b7c932e1f3bff4d7255dc2bcbe9a72c883761\nSuccessfully built gcpds-image-segmentation\nInstalling collected packages: gdown, gcpds-image-segmentation\nSuccessfully installed gcpds-image-segmentation-0.1a0 gdown-5.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#!pip install tensorflow==2.8","metadata":{"id":"LPXR0Q3MJKuA","executionInfo":{"status":"ok","timestamp":1697987811617,"user_tz":-180,"elapsed":86288,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"}},"outputId":"261c6a5e-9af8-483a-9ab2-85ebde1e0f45","execution":{"iopub.status.busy":"2025-01-16T14:26:25.355354Z","iopub.execute_input":"2025-01-16T14:26:25.355957Z","iopub.status.idle":"2025-01-16T14:26:25.360927Z","shell.execute_reply.started":"2025-01-16T14:26:25.355921Z","shell.execute_reply":"2025-01-16T14:26:25.359856Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!pip install scikeras[tensorflow] >/dev/null","metadata":{"id":"MYlk2_z0189s","executionInfo":{"status":"ok","timestamp":1697987885657,"user_tz":-180,"elapsed":74057,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"}},"outputId":"a3b676ec-17f3-4e49-fe20-5c4f635ad485","execution":{"iopub.status.busy":"2025-01-16T14:26:25.362204Z","iopub.execute_input":"2025-01-16T14:26:25.362702Z","iopub.status.idle":"2025-01-16T14:26:25.441296Z","shell.execute_reply.started":"2025-01-16T14:26:25.362653Z","shell.execute_reply":"2025-01-16T14:26:25.439998Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!git clone https://github.com/hdperezn/MIDI_functions.git\n!sudo apt install -y fluidsynth\n!pip install --upgrade pyfluidsynth\n!pip install pretty_midi","metadata":{"executionInfo":{"elapsed":36397,"status":"ok","timestamp":1697987922033,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"},"user_tz":-180},"id":"V9y68jZn1ykN","outputId":"9e8e204d-aebe-4420-d1cf-98eabf8a1584","execution":{"iopub.status.busy":"2025-01-16T14:26:25.444688Z","iopub.execute_input":"2025-01-16T14:26:25.445086Z","iopub.status.idle":"2025-01-16T14:27:27.974315Z","shell.execute_reply.started":"2025-01-16T14:26:25.445047Z","shell.execute_reply":"2025-01-16T14:27:27.972999Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'MIDI_functions'...\nremote: Enumerating objects: 185, done.\u001b[K\nremote: Counting objects: 100% (185/185), done.\u001b[K\nremote: Compressing objects: 100% (169/169), done.\u001b[K\nremote: Total 185 (delta 77), reused 71 (delta 11), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (185/185), 8.40 MiB | 16.28 MiB/s, done.\nResolving deltas: 100% (77/77), done.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  fluid-soundfont-gm libdouble-conversion3 libegl-mesa0 libegl1 libevdev2\n  libfluidsynth2 libgbm1 libgudev-1.0-0 libinput-bin libinput10\n  libinstpatch-1.0-2 libmtdev1 libpcre2-16-0 libqt5core5a libqt5dbus5\n  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n  libwacom-common libwacom2 libwayland-server0 libxcb-icccm4 libxcb-image0\n  libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0\n  libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme\n  qttranslations5-l10n timgm6mb-soundfont\nSuggested packages:\n  fluid-soundfont-gs timidity qt5-image-formats-plugins qtwayland5 jackd\n  musescore\nThe following NEW packages will be installed:\n  fluid-soundfont-gm fluidsynth libdouble-conversion3 libegl-mesa0 libegl1\n  libevdev2 libfluidsynth2 libgbm1 libgudev-1.0-0 libinput-bin libinput10\n  libinstpatch-1.0-2 libmtdev1 libpcre2-16-0 libqt5core5a libqt5dbus5\n  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n  libwacom-common libwacom2 libwayland-server0 libxcb-icccm4 libxcb-image0\n  libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0\n  libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme\n  qttranslations5-l10n timgm6mb-soundfont\n0 upgraded, 37 newly installed, 0 to remove and 73 not upgraded.\nNeed to get 136 MB of archives.\nAfter this operation, 204 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdouble-conversion3 amd64 3.1.5-4ubuntu1 [37.9 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpcre2-16-0 amd64 10.34-7ubuntu0.1 [181 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libqt5core5a amd64 5.12.8+dfsg-0ubuntu2.1 [2006 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwayland-server0 amd64 1.18.0-1ubuntu0.1 [31.3 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgbm1 amd64 21.2.6-0ubuntu0.1~20.04.2 [29.2 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libegl-mesa0 amd64 21.2.6-0ubuntu0.1~20.04.2 [96.3 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libegl1 amd64 1.3.2-1~ubuntu0.20.04.2 [31.9 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libevdev2 amd64 1.9.0+dfsg-1ubuntu0.2 [31.6 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libmtdev1 amd64 1.1.5-1.1 [14.2 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libgudev-1.0-0 amd64 1:233-1 [14.0 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwacom-common all 1.3-2ubuntu3 [45.3 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwacom2 amd64 1.3-2ubuntu3 [20.0 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libinput-bin amd64 1.15.5-1ubuntu0.3 [19.3 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libinput10 amd64 1.15.5-1ubuntu0.3 [112 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libqt5dbus5 amd64 5.12.8+dfsg-0ubuntu2.1 [208 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libqt5network5 amd64 5.12.8+dfsg-0ubuntu2.1 [673 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-icccm4 amd64 0.4.1-1.1 [10.8 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-util1 amd64 0.4.0-0ubuntu3 [11.2 kB]\nGet:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-image0 amd64 0.4.0-1build1 [12.3 kB]\nGet:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-keysyms1 amd64 0.4.0-1build1 [8452 B]\nGet:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-render-util0 amd64 0.3.9-1build1 [9912 B]\nGet:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-xinerama0 amd64 1.14-2 [5260 B]\nGet:23 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-xinput0 amd64 1.14-2 [29.3 kB]\nGet:24 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-xkb1 amd64 1.14-2 [29.6 kB]\nGet:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbcommon-x11-0 amd64 0.10.0-1 [13.4 kB]\nGet:26 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libqt5gui5 amd64 5.12.8+dfsg-0ubuntu2.1 [2971 kB]\nGet:27 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libqt5widgets5 amd64 5.12.8+dfsg-0ubuntu2.1 [2295 kB]\nGet:28 http://archive.ubuntu.com/ubuntu focal/universe amd64 libqt5svg5 amd64 5.12.8-0ubuntu1 [131 kB]\nGet:29 http://archive.ubuntu.com/ubuntu focal/universe amd64 fluid-soundfont-gm all 3.1-5.1 [119 MB]\nGet:30 http://archive.ubuntu.com/ubuntu focal/universe amd64 libinstpatch-1.0-2 amd64 1.1.2-2build1 [238 kB]\nGet:31 http://archive.ubuntu.com/ubuntu focal/universe amd64 timgm6mb-soundfont all 1.3-3 [5420 kB]\nGet:32 http://archive.ubuntu.com/ubuntu focal/universe amd64 libfluidsynth2 amd64 2.1.1-2 [198 kB]\nGet:33 http://archive.ubuntu.com/ubuntu focal/universe amd64 fluidsynth amd64 2.1.1-2 [25.6 kB]\nGet:34 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwacom-bin amd64 1.3-2ubuntu3 [5484 B]\nGet:35 http://archive.ubuntu.com/ubuntu focal/universe amd64 qsynth amd64 0.6.1-1build1 [245 kB]\nGet:36 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 qt5-gtk-platformtheme amd64 5.12.8+dfsg-0ubuntu2.1 [124 kB]\nGet:37 http://archive.ubuntu.com/ubuntu focal/universe amd64 qttranslations5-l10n all 5.12.8-0ubuntu1 [1486 kB]\nFetched 136 MB in 6s (24.7 MB/s)               \u001b[0m\u001b[33m\nExtracting templates from packages: 100%\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libdouble-conversion3:amd64.\n(Reading database ... 108782 files and directories currently installed.)\nPreparing to unpack .../00-libdouble-conversion3_3.1.5-4ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libdouble-conversion3:amd64 (3.1.5-4ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package libpcre2-16-0:amd64.\nPreparing to unpack .../01-libpcre2-16-0_10.34-7ubuntu0.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libpcre2-16-0:amd64 (10.34-7ubuntu0.1) ...\nSelecting previously unselected package libqt5core5a:amd64.\nPreparing to unpack .../02-libqt5core5a_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libqt5core5a:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libwayland-server0:amd64.\nPreparing to unpack .../03-libwayland-server0_1.18.0-1ubuntu0.1_amd64.deb ...\nUnpacking libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libgbm1:amd64.\nPreparing to unpack .../04-libgbm1_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libegl-mesa0:amd64.\nPreparing to unpack .../05-libegl-mesa0_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libegl1:amd64.\nPreparing to unpack .../06-libegl1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libevdev2:amd64.\nPreparing to unpack .../07-libevdev2_1.9.0+dfsg-1ubuntu0.2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libevdev2:amd64 (1.9.0+dfsg-1ubuntu0.2) ...\nSelecting previously unselected package libmtdev1:amd64.\nPreparing to unpack .../08-libmtdev1_1.1.5-1.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libmtdev1:amd64 (1.1.5-1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libgudev-1.0-0:amd64.\nPreparing to unpack .../09-libgudev-1.0-0_1%3a233-1_amd64.deb ...\nUnpacking libgudev-1.0-0:amd64 (1:233-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libwacom-common.\nPreparing to unpack .../10-libwacom-common_1.3-2ubuntu3_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libwacom-common (1.3-2ubuntu3) ...\nSelecting previously unselected package libwacom2:amd64.\nPreparing to unpack .../11-libwacom2_1.3-2ubuntu3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libwacom2:amd64 (1.3-2ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libinput-bin.\nPreparing to unpack .../12-libinput-bin_1.15.5-1ubuntu0.3_amd64.deb ...\nUnpacking libinput-bin (1.15.5-1ubuntu0.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libinput10:amd64.\nPreparing to unpack .../13-libinput10_1.15.5-1ubuntu0.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libinput10:amd64 (1.15.5-1ubuntu0.3) ...\nSelecting previously unselected package libqt5dbus5:amd64.\nPreparing to unpack .../14-libqt5dbus5_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libqt5dbus5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libqt5network5:amd64.\nPreparing to unpack .../15-libqt5network5_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\nUnpacking libqt5network5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libxcb-icccm4:amd64.\nPreparing to unpack .../16-libxcb-icccm4_0.4.1-1.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libxcb-icccm4:amd64 (0.4.1-1.1) ...\nSelecting previously unselected package libxcb-util1:amd64.\nPreparing to unpack .../17-libxcb-util1_0.4.0-0ubuntu3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libxcb-util1:amd64 (0.4.0-0ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libxcb-image0:amd64.\nPreparing to unpack .../18-libxcb-image0_0.4.0-1build1_amd64.deb ...\nUnpacking libxcb-image0:amd64 (0.4.0-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libxcb-keysyms1:amd64.\nPreparing to unpack .../19-libxcb-keysyms1_0.4.0-1build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libxcb-keysyms1:amd64 (0.4.0-1build1) ...\nSelecting previously unselected package libxcb-render-util0:amd64.\nPreparing to unpack .../20-libxcb-render-util0_0.3.9-1build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libxcb-render-util0:amd64 (0.3.9-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libxcb-xinerama0:amd64.\nPreparing to unpack .../21-libxcb-xinerama0_1.14-2_amd64.deb ...\nUnpacking libxcb-xinerama0:amd64 (1.14-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libxcb-xinput0:amd64.\nPreparing to unpack .../22-libxcb-xinput0_1.14-2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libxcb-xinput0:amd64 (1.14-2) ...\nSelecting previously unselected package libxcb-xkb1:amd64.\nPreparing to unpack .../23-libxcb-xkb1_1.14-2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libxcb-xkb1:amd64 (1.14-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libxkbcommon-x11-0:amd64.\nPreparing to unpack .../24-libxkbcommon-x11-0_0.10.0-1_amd64.deb ...\nUnpacking libxkbcommon-x11-0:amd64 (0.10.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libqt5gui5:amd64.\nPreparing to unpack .../25-libqt5gui5_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libqt5gui5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\nSelecting previously unselected package libqt5widgets5:amd64.\nPreparing to unpack .../26-libqt5widgets5_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking libqt5widgets5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libqt5svg5:amd64.\nPreparing to unpack .../27-libqt5svg5_5.12.8-0ubuntu1_amd64.deb ...\nUnpacking libqt5svg5:amd64 (5.12.8-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package fluid-soundfont-gm.\nPreparing to unpack .../28-fluid-soundfont-gm_3.1-5.1_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking fluid-soundfont-gm (3.1-5.1) ...\nSelecting previously unselected package libinstpatch-1.0-2:amd64.\nPreparing to unpack .../29-libinstpatch-1.0-2_1.1.2-2build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libinstpatch-1.0-2:amd64 (1.1.2-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package timgm6mb-soundfont.\nPreparing to unpack .../30-timgm6mb-soundfont_1.3-3_all.deb ...\nUnpacking timgm6mb-soundfont (1.3-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libfluidsynth2:amd64.\nPreparing to unpack .../31-libfluidsynth2_2.1.1-2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libfluidsynth2:amd64 (2.1.1-2) ...\nSelecting previously unselected package fluidsynth.\nPreparing to unpack .../32-fluidsynth_2.1.1-2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking fluidsynth (2.1.1-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libwacom-bin.\nPreparing to unpack .../33-libwacom-bin_1.3-2ubuntu3_amd64.deb ...\nUnpacking libwacom-bin (1.3-2ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package qsynth.\nPreparing to unpack .../34-qsynth_0.6.1-1build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking qsynth (0.6.1-1build1) ...\nSelecting previously unselected package qt5-gtk-platformtheme:amd64.\nPreparing to unpack .../35-qt5-gtk-platformtheme_5.12.8+dfsg-0ubuntu2.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking qt5-gtk-platformtheme:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [############################..............................] \u001b8Selecting previously unselected package qttranslations5-l10n.\nPreparing to unpack .../36-qttranslations5-l10n_5.12.8-0ubuntu1_all.deb ...\nUnpacking qttranslations5-l10n (5.12.8-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libdouble-conversion3:amd64 (3.1.5-4ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libxcb-xinput0:amd64 (1.14-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libxcb-keysyms1:amd64 (0.4.0-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libxcb-render-util0:amd64 (0.3.9-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libxcb-icccm4:amd64 (0.4.1-1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libpcre2-16-0:amd64 (10.34-7ubuntu0.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libxcb-util1:amd64 (0.4.0-0ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libxcb-xkb1:amd64 (1.14-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libxcb-image0:amd64 (0.4.0-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxcb-xinerama0:amd64 (1.14-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up qttranslations5-l10n (5.12.8-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libxkbcommon-x11-0:amd64 (0.10.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libqt5core5a:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libmtdev1:amd64 (1.1.5-1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libqt5dbus5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up fluid-soundfont-gm (3.1-5.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up timgm6mb-soundfont (1.3-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\nupdate-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\nSetting up libevdev2:amd64 (1.9.0+dfsg-1ubuntu0.2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libinstpatch-1.0-2:amd64 (1.1.2-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libgudev-1.0-0:amd64 (1:233-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libwacom-common (1.3-2ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libqt5network5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libfluidsynth2:amd64 (2.1.1-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libwacom2:amd64 (1.3-2ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libinput-bin (1.15.5-1ubuntu0.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up fluidsynth (2.1.1-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Created symlink /etc/systemd/user/multi-user.target.wants/fluidsynth.service -> /usr/lib/systemd/user/fluidsynth.service.\nSetting up libwacom-bin (1.3-2ubuntu3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libinput10:amd64 (1.15.5-1ubuntu0.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libqt5gui5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libqt5widgets5:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up qt5-gtk-platformtheme:amd64 (5.12.8+dfsg-0ubuntu2.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libqt5svg5:amd64 (5.12.8-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up qsynth (0.6.1-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for mime-support (3.64ubuntu1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.9) ...\nProcessing triggers for man-db (2.9.1-1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JCollecting pyfluidsynth\n  Obtaining dependency information for pyfluidsynth from https://files.pythonhosted.org/packages/c4/91/4f6b28ac379da306dde66ba6ac170c4a6e7e1506cadc84a9359fe3f237ba/pyfluidsynth-1.3.4-py3-none-any.whl.metadata\n  Downloading pyfluidsynth-1.3.4-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pyfluidsynth) (1.24.3)\nDownloading pyfluidsynth-1.3.4-py3-none-any.whl (22 kB)\nInstalling collected packages: pyfluidsynth\nSuccessfully installed pyfluidsynth-1.3.4\nCollecting pretty_midi\n  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from pretty_midi) (1.24.3)\nCollecting mido>=1.1.16 (from pretty_midi)\n  Obtaining dependency information for mido>=1.1.16 from https://files.pythonhosted.org/packages/fd/28/45deb15c11859d2f10702b32e71de9328a9fa494f989626916db39a9617f/mido-1.3.3-py3-none-any.whl.metadata\n  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from pretty_midi) (1.16.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mido>=1.1.16->pretty_midi) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mido>=1.1.16->pretty_midi) (3.0.9)\nDownloading mido-1.3.3-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pretty_midi\n  Building wheel for pretty_midi (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=7bacec2e7a8da0bc1759534f9f749e3ff1bb8029672e6a3215dc404f240c8dcb\n  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\nSuccessfully built pretty_midi\nInstalling collected packages: mido, pretty_midi\nSuccessfully installed mido-1.3.3 pretty_midi-0.2.10\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport math\nimport IPython\nimport string\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n\nimport fluidsynth\nimport glob\nimport pathlib\nimport pretty_midi\n\nfrom IPython import display\nfrom scipy.spatial.distance import cdist\nfrom MIDI_functions.functions.MIDI_functions import cut_midi_secTrial,piano_roll_to_pretty_midi\nfrom MIDI_functions.functions.UNetLike_PianoRolls import UNet_Pianoroll, Autoencoder_Pianoroll\n\n\nfrom tensorflow.keras.regularizers import l1_l2\nfrom sklearn.model_selection import StratifiedShuffleSplit\n#from scikeras.wrappers import KerasClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n","metadata":{"id":"zfkQkRSG15b9","execution":{"iopub.status.busy":"2025-01-16T14:27:27.976222Z","iopub.execute_input":"2025-01-16T14:27:27.976671Z","iopub.status.idle":"2025-01-16T14:27:29.001872Z","shell.execute_reply.started":"2025-01-16T14:27:27.976627Z","shell.execute_reply":"2025-01-16T14:27:29.000595Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# some usefull functions (window EEG and custom loss) and paths","metadata":{"id":"KMQP3j1ak7Zn"}},{"cell_type":"code","source":"#Paths\npath_DEAP = '/kaggle/input/deap-dataset'\n#save_path = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/'\nvideo_list = pd.read_excel(\"/kaggle/input/d/manh123df/deap-dataset/deap-dataset/Metadata/video_list_fixed.xlsx\") #Metadata/video_list_fixed.xlsx\nvideo_list_experiment = video_list.dropna(subset= ['Experiment_id'])\n# path to load EEGnet weigths\nEEGnet_weigths_path = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/EEG/'\n\n## trials out\ntrials_toCUT_inEEG = np.asarray([1, 18,19, 22,23, 27, 33, 43, 79])","metadata":{"id":"XwBGwoq96rCD","execution":{"iopub.status.busy":"2025-01-16T14:30:41.665986Z","iopub.execute_input":"2025-01-16T14:30:41.666408Z","iopub.status.idle":"2025-01-16T14:30:42.166305Z","shell.execute_reply.started":"2025-01-16T14:30:41.666366Z","shell.execute_reply":"2025-01-16T14:30:42.164989Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def segmentation_trials(X, fs, segs= 6):\n  \"\"\"\n  Input\n  X: numpy array with EEG info, dims (trials,channels,time)\n  fs: int value with the sampling frequency\n  segs: int vale with the length of the trial\n\n  out\n  X_train: numpy array with the eeg data windowed in trials with length \"segs\"\n  \"\"\"\n\n  X_train = []\n  windows = int(X.shape[2]/(segs*fs))\n  for i in range(X.shape[0]):\n    for j in range(windows):\n        #print(j*(6*fs),int((j+1)*(segs*fs)))\n        X_train.append(X[i,:,j*(segs*fs):int((j+1)*(segs*fs))])\n  return np.asarray(X_train)","metadata":{"id":"6grwW-ndkYVH","execution":{"iopub.status.busy":"2025-01-16T14:30:51.697426Z","iopub.execute_input":"2025-01-16T14:30:51.698302Z","iopub.status.idle":"2025-01-16T14:30:51.707409Z","shell.execute_reply.started":"2025-01-16T14:30:51.698263Z","shell.execute_reply":"2025-01-16T14:30:51.705812Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def pading_midis(Prolls_windowed, Prolls_trial_len = 128):\n  \"\"\"\n  function to cut or padding the midi arrays to equal lengs\n  \"\"\"\n  Prolls_cut = []\n  for i in range(len(Prolls_windowed)):\n    if Prolls_windowed[i].shape[2] >= Prolls_trial_len:\n      x = Prolls_windowed[i][:,:,0:Prolls_trial_len]\n      Prolls_cut.append(x)\n    else:\n      dif = Prolls_trial_len - Prolls_windowed[i].shape[2]\n      x = np.concatenate( (Prolls_windowed[i][:,:,:],Prolls_windowed[i][:,:,-dif::] ), axis = 2)\n      Prolls_cut.append(x)\n  return np.concatenate(np.array(Prolls_cut, dtype=object), axis = 0)\n","metadata":{"id":"4YfjPj0LUcqU","execution":{"iopub.status.busy":"2025-01-16T14:30:51.710559Z","iopub.execute_input":"2025-01-16T14:30:51.710927Z","iopub.status.idle":"2025-01-16T14:30:51.73363Z","shell.execute_reply.started":"2025-01-16T14:30:51.710897Z","shell.execute_reply":"2025-01-16T14:30:51.732291Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def pearson_multidims(y_true, y_pred ):\n  num = tf.linalg.trace(tf.linalg.matmul(y_true,y_pred, transpose_a=True))\n  # -------------------------\n  dem1 = tf.linalg.trace(tf.linalg.matmul(y_true,y_true, transpose_a=True))\n  dem2 = tf.linalg.trace(tf.linalg.matmul(y_pred,y_pred, transpose_a=True))\n  dem = tf.sqrt(dem1)*tf.sqrt(dem2)\n  return num/dem\n\ndef custom_lossRho(y_true, y_pred):\n  # -------------------------Pearson loss----------------------------\n  mean_point=y_pred.shape[-1]//2\n  phyEEG=tf.cast(y_pred[:,0:mean_point], tf.float32)\n  phyenco=tf.cast(y_pred[:,mean_point:], tf.float32)\n  return pearson_multidims(phyEEG, phyenco)","metadata":{"id":"Mi3RK8ki4_hZ","execution":{"iopub.status.busy":"2025-01-16T14:30:51.73506Z","iopub.execute_input":"2025-01-16T14:30:51.735494Z","iopub.status.idle":"2025-01-16T14:30:51.752801Z","shell.execute_reply.started":"2025-01-16T14:30:51.735461Z","shell.execute_reply":"2025-01-16T14:30:51.751607Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def split_EEGNet(eegnet, layer_split = 11):\n  # ----------- encoding layers\n  imput_encoding = tf.keras.layers.Input(shape = (Chans, Samples, 1))\n  layer = imput_encoding\n  for i in range(1,layer_split):\n      layer = eegnet.layers[i](layer)\n  EEGNet_encoding_layer = tf.keras.models.Model(inputs=imput_encoding, outputs=layer)\n  EEGNet_encoding_layer.trainable == False\n  # ----------- clasification layers\n  imput_clasification = tf.keras.layers.Input(shape = layer.shape[1::])\n  layer = imput_clasification\n  for i in range(layer_split,len(eegnet.layers)):\n      layer = eegnet.layers[i](layer)\n  EEGNet_clasification_layer = tf.keras.models.Model(inputs=imput_clasification, outputs=layer)\n  EEGNet_clasification_layer.trainable == False\n\n  return EEGNet_encoding_layer, EEGNet_clasification_layer","metadata":{"id":"0c0O3Sysc0Cj","execution":{"iopub.status.busy":"2025-01-16T14:30:51.754799Z","iopub.execute_input":"2025-01-16T14:30:51.755188Z","iopub.status.idle":"2025-01-16T14:30:51.769316Z","shell.execute_reply.started":"2025-01-16T14:30:51.755155Z","shell.execute_reply":"2025-01-16T14:30:51.768039Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def rebuilPianoRolls(X):\n  \"\"\"\n  this function takes an output array from the vae net and returns\n  a piano roll array ready to be converted into MIDI file\n  input\n  X: array with dimensions [samples, time, pitch, 1]\n  output\n  proll_padding: array with dimension [sample x pitch x time ]\n  \"\"\"\n  #trasnpose to [sample x time x pitch]\n  proll = X[:,:,:,0].transpose(0,2,1)\n  proll_padding =np.pad(proll, ((0,0), (24,40), (0, 0)), 'constant')\n  return proll_padding","metadata":{"id":"I5wM4p6_b1A0","execution":{"iopub.status.busy":"2025-01-16T14:30:51.772403Z","iopub.execute_input":"2025-01-16T14:30:51.772765Z","iopub.status.idle":"2025-01-16T14:30:51.788842Z","shell.execute_reply.started":"2025-01-16T14:30:51.772737Z","shell.execute_reply":"2025-01-16T14:30:51.787569Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Create Piano roll train, test files","metadata":{"id":"vIxDNt0qPS4W"}},{"cell_type":"code","source":"path_midi = '/kaggle/working/MIDI_functions/deap_midis'\ndata_dir = pathlib.Path(path_midi)\nfilenames = glob.glob(str(data_dir/'*.mid*'))\nprint('Number of files:', len(filenames))","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1697915773443,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"},"user_tz":-180},"id":"9Ly_n-o0huDZ","outputId":"746ea653-47a0-4042-83f7-0fcbe5595ce2","execution":{"iopub.status.busy":"2025-01-16T14:30:51.790284Z","iopub.execute_input":"2025-01-16T14:30:51.790618Z","iopub.status.idle":"2025-01-16T14:30:51.802937Z","shell.execute_reply.started":"2025-01-16T14:30:51.790592Z","shell.execute_reply":"2025-01-16T14:30:51.801711Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of files: 40\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"_SAMPLING_RATE = 16000\nfProll = 20","metadata":{"id":"Rn6yXUGCP0uf","execution":{"iopub.status.busy":"2025-01-16T14:30:51.804344Z","iopub.execute_input":"2025-01-16T14:30:51.804716Z","iopub.status.idle":"2025-01-16T14:30:51.817417Z","shell.execute_reply.started":"2025-01-16T14:30:51.804689Z","shell.execute_reply":"2025-01-16T14:30:51.815928Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"Prolls_windowed_train = []\nfor i in range(len(filenames)):\n  #load MIDI and create array\n  Proll = pretty_midi.PrettyMIDI(filenames[i]).get_piano_roll(fs=fProll)\n  if i in [0, 13, 16, 21, 39]:\n    Proll_cut = cut_midi_secTrial(Proll, 9)\n  elif i in [9, 11]:\n    Proll_cut = cut_midi_secTrial(Proll, 8)\n  else:\n    Proll_cut = cut_midi_secTrial(Proll, 10)\n  Prolls_windowed_train.append(Proll_cut[:])\n\nProlls_array_train = pading_midis(Prolls_windowed_train, Prolls_trial_len = 128)\nProlls_train_mask = np.where(Prolls_array_train>0,1,0)\n\nprint('these are de dims of the MIDI data to use in the neural network: ')\nprint('     train data:', Prolls_array_train.shape)\n","metadata":{"executionInfo":{"elapsed":1572,"status":"ok","timestamp":1697915775011,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"},"user_tz":-180},"id":"zySTW6UBxqMv","outputId":"550fd9ec-7140-4d23-859f-5d631c79f1e6","execution":{"iopub.status.busy":"2025-01-16T14:30:51.818795Z","iopub.execute_input":"2025-01-16T14:30:51.819278Z","iopub.status.idle":"2025-01-16T14:30:52.483601Z","shell.execute_reply.started":"2025-01-16T14:30:51.819241Z","shell.execute_reply":"2025-01-16T14:30:52.482414Z"},"trusted":true},"outputs":[{"name":"stdout","text":"these are de dims of the MIDI data to use in the neural network: \n     train data: (391, 128, 128)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"indx_full_audio = []#np.zeros(len(filenames))\nall_trials = np.array(range(400))\nfor i in range(len(filenames)):\n  #load MIDI and create array\n  if i in [0, 13, 16, 21, 39]:\n    indx_full_audio.extend([1,1,1,1,1,1,1,1,1,np.nan])\n  elif i in [9, 11]:\n    indx_full_audio.extend([1,1,1,1,1,1,1,1,np.nan,np.nan])\n  else:\n    indx_full_audio.extend([1,1,1,1,1,1,1,1,1,1])\nindx_full_audio = all_trials*np.array(indx_full_audio)\n\nindx_full_audio = indx_full_audio[~np.isnan(indx_full_audio)].astype(int)\n","metadata":{"id":"Ko_slgqlm6j3","execution":{"iopub.status.busy":"2025-01-16T14:30:52.484972Z","iopub.execute_input":"2025-01-16T14:30:52.48535Z","iopub.status.idle":"2025-01-16T14:30:52.493529Z","shell.execute_reply.started":"2025-01-16T14:30:52.485307Z","shell.execute_reply":"2025-01-16T14:30:52.492007Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# save_models_path = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/EEG/cross_val_results/'\n# mask_probs = np.where(np.load(save_models_path+'top_20_mask_probas.npy') > 0.5, 1, 0)\n\n# Prolls_windowed_train = []\n# for i in range(len(filenames)):\n#   #load MIDI and create array\n#   mask_song = mask_probs[i]\n\n#   Proll = pretty_midi.PrettyMIDI(filenames[i]).get_piano_roll(fs=fProll)\n#   if i in [0, 13, 16, 21, 39]:\n#     mask = mask_song[0:9] == 1\n#     Proll_cut = cut_midi_secTrial(Proll, 9)[mask]\n#   elif i in [9, 11]:\n#     mask = mask_song[0:8] == 1\n#     Proll_cut = cut_midi_secTrial(Proll, 8)[mask]\n#   else:\n#     mask = mask_song == 1\n#     Proll_cut = cut_midi_secTrial(Proll, 10)[mask]\n\n#   Prolls_windowed_train.append(Proll_cut[:])\n\n# Prolls_array_train = pading_midis(Prolls_windowed_train, Prolls_trial_len = 128)\n# Prolls_train_mask = np.where(Prolls_array_train>0,1,0)\n\n# print('these are de dims of the MIDI data to use in the neural network: ')\n# print('     train data:', Prolls_array_train.shape)\n\n# #train_test_data\n# Xpr_train = Prolls_array_train[:,24:88,:,np.newaxis].astype('float32').transpose(0,2,1,3)\n# ypr_train = Prolls_train_mask[:,24:88,:,np.newaxis].astype('float32').transpose(0,2,1,3)\n# Xpr_dims = Xpr_train.shape\n# print('EEG variables names: \"X_train\"', Xpr_train.shape )\n# print('MIDI variables names: \"Xpr_train\"')\n\n\n# indx_full_audio = []#np.zeros(len(filenames))\n# all_trials = np.array(range(400))\n# for i in range(len(filenames)):\n#   mask_song = mask_probs[i]\n#   #load MIDI and create array\n#   if i in [0, 13, 16, 21, 39]:\n#     list_mask = np.array([1,1,1,1,1,1,1,1,1,np.nan])\n#     idx_mask = np.where(mask_song == 0, np.nan, list_mask)\n\n#     indx_full_audio.extend(idx_mask)\n\n#   elif i in [9, 11]:\n#     list_mask = np.array([1,1,1,1,1,1,1,1,np.nan,np.nan])\n#     idx_mask = np.where(mask_song == 0, np.nan, list_mask)\n\n#     indx_full_audio.extend(idx_mask)\n#   else:\n#     list_mask = np.array([1,1,1,1,1,1,1,1,1,1])\n#     idx_mask = np.where(mask_song == 0, np.nan, list_mask)\n#     indx_full_audio.extend(idx_mask)\n\n# indx_full_audio = all_trials*np.array(indx_full_audio)\n\n# indx_full_audio = indx_full_audio[~np.isnan(indx_full_audio)].astype(int)\n","metadata":{"id":"9ewAHvgEHC_M","execution":{"iopub.status.busy":"2025-01-16T14:30:52.495352Z","iopub.execute_input":"2025-01-16T14:30:52.495698Z","iopub.status.idle":"2025-01-16T14:30:52.507427Z","shell.execute_reply.started":"2025-01-16T14:30:52.49567Z","shell.execute_reply":"2025-01-16T14:30:52.506315Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Functions","metadata":{"id":"P1DKeid6aulr"}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l1_l2\nfrom sklearn.base import  BaseEstimator, TransformerMixin, ClassifierMixin\nimport tensorflow_probability as tfp\nclass autoencoderCKA_Pianoroll(BaseEstimator, ClassifierMixin):\n  def __init__(self, img_size, loss, num_classes = 1, labels = 2, epochs=50,batch_size=32, Q1= 20, Q2=2,\n               learning_rate=1e-3,validation_split=0.2,verbose=1, droprate = 0.5, filters_list = [128, 64, 32],\n               l1_l2 = 0,  Dice_weigths = [0.5,0.5],  plot_loss= True):\n\n    self.img_size = img_size\n    self.labels = labels\n    self.loss = loss #must be two losses\n    self.num_classes = num_classes\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.learning_rate=learning_rate\n    self.validation_split = validation_split\n    self.verbose = verbose\n    self.droprate = droprate\n    self.plot_loss = plot_loss\n    self.filters_list = filters_list\n    self.Dice_weigths = Dice_weigths\n    self.l1_l2 = l1_l2\n\n    self.Q1 = Q1\n    self.Q2 = Q2\n\n  def Encoder(self, img_size ):\n    self.encoder_inputs = tf.keras.Input(shape=(img_size))\n    ### [First half of the network: downsampling inputs] ###\n    filters = self.filters_list[::-1]\n\n    # Entry block\n    x = layers.Conv2D(filters[0], 3, strides=2, padding=\"same\", name = 'conv_filter32_1')(self.encoder_inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for fil in filters[1::]:\n        x = layers.Dropout(self.droprate)(x)\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(fil, 3, padding=\"same\", name= 'Enco_conv_filter'+str(fil)+'_2')(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(fil, 3, padding=\"same\", name= 'Enco_conv_filter'+str(fil)+'_3',\n                                   kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(fil, 1, strides=2, padding=\"same\", name= 'Enco_conv_filter'+str(fil)+'_4'\n        , kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(previous_block_activation )\n        x = layers.add([x, residual], name = 'Enco_add_block_filter'+str(fil))  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(fil, 3, strides= 2, padding=\"same\", name= 'Enco_conv_filter'+str(fil)+'_5',\n                                   kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(x)\n    self.x_nonflat = x\n    x = layers.Flatten()(x)\n    self.x = x\n    self.encoder = tf.keras.Model(self.encoder_inputs, outputs = [self.x], name=\"encoder\")\n    return self.encoder\n\n  def Decoder(self, num_classes):\n    input_x = tf.keras.Input(shape=(int(self.x.shape[1])))\n\n    #x = layers.Dense(int(8* 4* 128), activation='selu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1_l2,l2=self.l1_l2))(input_x)\n    #x = input_x\n    x = layers.Reshape((8, 4, 128))(input_x)\n\n    x = layers.Conv2DTranspose(128, 3, strides= 2, padding=\"same\", name= 'Deco_conv_filter'+str(128)+'_0',\n                            kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(x)\n    previous_block_activation = x\n\n    for filters in self.filters_list:\n        x = layers.Dropout(self.droprate)(x)\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\", name= 'Deco_conv_filter'+str(filters)+'_1')(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\", name= 'Deco_conv_filter'+str(filters)+'_2',\n                                   kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\", name= 'Deco_conv_filter'+str(filters)+'_3',\n                                 kernel_regularizer=tf.keras.regularizers.L1L2(l1=self.l1_l2, l2=self.l1_l2))(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    self.outputs = layers.Conv2D(num_classes, 3, activation=\"sigmoid\", padding=\"same\")(x)\n    self.decoder = tf.keras.Model(inputs= [input_x], outputs = self.outputs, name=\"Decoder\")\n    return self.decoder\n\n  def CKA_layers(self):\n    regularizer_o = tf.keras.regularizers.OrthogonalRegularizer(factor=self.l1_l2)\n    input_x = tf.keras.Input(shape=(self.x.shape[1]))\n\n    #flattenA = tf.keras.layers.Flatten()(input_x)\n    flattenA = layers.BatchNormalization()(input_x)\n\n    h1A = layers.Dropout(self.droprate)(flattenA)\n    h1A = tf.keras.layers.Dense(self.Q1,activation='selu',name='h1A',\n                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1_l2,l2=self.l1_l2))(h1A)\n    h1A = layers.BatchNormalization()(h1A)\n    h2A = tf.keras.layers.Dense(self.Q2,activation='linear',name='h2A',\n                                kernel_regularizer=regularizer_o)(h1A)\n    h2A = layers.BatchNormalization()(h2A)\n\n    self.model = tf.keras.Model(inputs=[input_x],outputs=[h2A], name = 'cka_PianoRoll')\n    return self.model\n\n\n  def get_model(self, *_):\n    seed = 123\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    tf.keras.backend.clear_session()\n\n    self.winitializer = tf.keras.initializers.GlorotNormal(seed=seed)\n    self.binitializer = \"zeros\"\n    # ---- call layers -----\n    enco = self.Encoder(self.img_size)\n    deco = self.Decoder(self.num_classes)\n    cka = self.CKA_layers()\n    # ---- def red ---------\n    block = enco(self.encoder_inputs)\n    decoder_ = deco(block)\n    cka_out = cka(block)\n\n\n    # ----- MODEL -------\n\n    metris = [tf.keras.metrics.Recall(), tf.keras.metrics.SpecificityAtSensitivity(0.1)]\n\n    self.model = tf.keras.Model(inputs=[self.encoder_inputs],  outputs=[decoder_,cka_out], name = 'AE_CKA_MIDI')\n    opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n    #binady cross entropy\n    #loss_classification = tf.keras.losses.BinaryCrossentropy()\n    self.model.compile(loss= self.loss , loss_weights = self.Dice_weigths,  optimizer= opt, metrics=metris)\n    return\n\n  def fit(self, X, Y, *_):\n    # Y must be a list with the MIDI and the label\n    callback = tf.keras.callbacks.TerminateOnNaN()\n\n    self.get_model()\n    self.history = self.model.fit(X, Y , epochs=self.epochs, batch_size=self.batch_size, validation_split=self.validation_split,\n                                  callbacks=[callback],\n                                  verbose=self.verbose)\n    # ----- plot loss -----\n    if self.plot_loss:\n          self.plt_history()\n\n  def predict(self, X,  *_):\n    return self.model.predict(X)\n  def plt_history(self):\n    plt.plot(self.history.history['loss'])\n    plt.plot(self.history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    return\n\n\nclass CKA_Pianoroll(BaseEstimator, ClassifierMixin):\n  def __init__(self, img_size, loss, piano_roll_model , CNNf = 4, Q1= 20, Q2=2, l1_l2 = 1e-3,\n               epochs=50, batch_size=32, learning_rate=1e-3, validation_split=0.2,\n               verbose=1, droprate = 0.5, plot_loss= True):\n    self.img_size = img_size\n    self.loss = loss\n\n    self.piano_roll_model = piano_roll_model\n    self.CNNf = CNNf\n    self.Q1 = Q1\n    self.Q2 = Q2\n    self.l1 = l1_l2\n    self.l2 = l1_l2\n\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.learning_rate=learning_rate\n    self.validation_split = validation_split\n    self.verbose = verbose\n    self.drop_rate = droprate\n    self.plot_loss = plot_loss\n\n  def get_model(self, *_):\n\n    # seed = 123\n    # tf.random.set_seed(seed)\n    # np.random.seed(seed)\n    # tf.keras.backend.clear_session()\n    regularizer_o = tf.keras.regularizers.OrthogonalRegularizer(factor=self.l2)\n\n    inputA = tf.keras.layers.Input(shape=(img_size), name='entradaA')\n    # convA = tf.keras.layers.Conv2D(self.CNNf, 5, strides=2, padding=\"same\", name = 'conv_filter_A1',activation='selu')(inputA)\n    # convA=  tf.keras.layers.MaxPooling2D(pool_size=2)(convA)\n    # convA = tf.keras.layers.Conv2D(2*self.CNNf, 3, strides=2, padding=\"same\", name = 'conv_filter_A2',activation='selu')(convA)\n    # convA=  tf.keras.layers.MaxPooling2D(pool_size=2)(convA)\n    # flattenA = tf.keras.layers.Flatten(input_shape=(convA.shape[1],convA.shape[2],convA.shape[3]))(convA)\n    enco_pr = self.piano_roll_model.encoder(inputA)\n    flattenA = layers.BatchNormalization()(enco_pr)\n    h1A = layers.Dropout(self.drop_rate)(flattenA)\n    h1A = tf.keras.layers.Dense(self.Q1,activation='selu',name='h1A',\n                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1,l2=self.l2))(h1A)\n    h1A = layers.BatchNormalization()(h1A)\n    h2A = tf.keras.layers.Dense(self.Q2,activation='linear',name='h2A',\n                                kernel_regularizer=regularizer_o)(h1A)\n    h2A = layers.BatchNormalization()(h2A)\n\n    self.model = tf.keras.Model(inputs=[inputA],outputs=[h2A], name = 'cka_PianoRoll')\n    return\n\n  def fit(self, Xtrain, ytrain,*_):\n\n    opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n    loss = self.loss\n\n    callback1 = tf.keras.callbacks.TerminateOnNaN()\n\n    self.get_model()\n\n    self.model.compile(loss=loss, optimizer=opt)\n    history = self.model.fit(x =[Xtrain], y=[ytrain],\n                            epochs=self.epochs,batch_size=self.batch_size, # 32, 64, 128, 256\n                             validation_split=self.validation_split,  callbacks=[callback1], verbose = self.verbose)\n\n\n    return history\n\n  def predict(self, X, *_):\n    return self.model.predict(X)\n\n  def plt_history(self):\n    plt.plot(self.history.history['loss'])\n    plt.plot(self.history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    return\n\n","metadata":{"id":"SDA39Xzoo9fW","execution":{"iopub.status.busy":"2025-01-16T14:30:52.509255Z","iopub.execute_input":"2025-01-16T14:30:52.509672Z","iopub.status.idle":"2025-01-16T14:30:55.112689Z","shell.execute_reply.started":"2025-01-16T14:30:52.50964Z","shell.execute_reply":"2025-01-16T14:30:55.111376Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\n# Define custom loss\n#centered kernel alignmet\n#http://www.jmlr.org/papers/volume13/cortes12a/cortes12a.pdf\n#https://www.frontiersin.org/articles/10.3389/fnins.2017.00550/full\ndef custom_loss(scaleX=1,scaleY=1):\n    #@tf.function()  #decorador para operar sobre python, mas lento y poco efectivo en muchos casos\n    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n    def custom_cka_loss(y_true,y_pred): #ytrue labels, ypred  = Xw\n        ####gradiente##########################################\n        y_true = tf.cast(y_true,dtype=tf.float32)\n        y_pred = tf.cast(y_pred,dtype=tf.float32)\n        scalar_kernel = tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude=1, length_scale= tf.cast(scaleX,dtype=tf.float32))\n        scalar_kernely = tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude=1, length_scale=tf.cast(scaleY,dtype=tf.float32))\n        k = scalar_kernel.matrix(y_pred, y_pred)\n        l = scalar_kernely.matrix(y_true, y_true)\n        ######################################################\n        N = tf.shape(l)[0]\n        N2 = tf.cast(tf.shape(l)[0],dtype=tf.float32)\n        h = tf.eye(N) - (1.0/N2)*tf.ones([N,1])*tf.ones([1,N]) #matrix for centered kernel\n        trkl = tf.linalg.trace(tf.matmul(tf.matmul(k,h),tf.matmul(l,h)))\n        trkk = tf.linalg.trace(tf.matmul(tf.matmul(k,h),tf.matmul(k,h)))\n        trll = tf.linalg.trace(tf.matmul(tf.matmul(l,h),tf.matmul(l,h)))\n        #####funcion de costo############################################3\n        f     = -trkl/tf.sqrt(trkk*trll)#-tf.math.log(trkl)+0.5*tf.math.log(trkk)## negative cka cost function (minimizing) f \\in [-1,0] #\n        return f\n    # Return a function\n    return custom_cka_loss","metadata":{"id":"ExrXPnDAfIiy","execution":{"iopub.status.busy":"2025-01-16T14:30:55.114776Z","iopub.execute_input":"2025-01-16T14:30:55.115206Z","iopub.status.idle":"2025-01-16T14:30:55.12589Z","shell.execute_reply.started":"2025-01-16T14:30:55.115171Z","shell.execute_reply":"2025-01-16T14:30:55.124641Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import Conv2D, AveragePooling2D\nfrom tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import SpatialDropout2D\nfrom tensorflow.keras.layers import Input, Flatten\nfrom tensorflow.keras.constraints import max_norm\n\ndef EEGNet_two_output(nb_classes, Chans = 64, Samples = 128,\n             dropoutRate = 0.5, kernLength = 64, F1 = 8,\n             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n    \"\"\" Keras Implementation of EEGNet\n    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n    Note that this implements the newest version of EEGNet and NOT the earlier\n    version (version v1 and v2 on arxiv). We strongly recommend using this\n    architecture as it performs much better and has nicer properties than\n    our earlier version. For example:\n\n        1. Depthwise Convolutions to learn spatial filters within a\n        temporal convolution. The use of the depth_multiplier option maps\n        exactly to the number of spatial filters learned within a temporal\n        filter. This matches the setup of algorithms like FBCSP which learn\n        spatial filters within each filter in a filter-bank. This also limits\n        the number of free parameters to fit when compared to a fully-connected\n        convolution.\n\n        2. Separable Convolutions to learn how to optimally combine spatial\n        filters across temporal bands. Separable Convolutions are Depthwise\n        Convolutions followed by (1x1) Pointwise Convolutions.\n\n\n    While the original paper used Dropout, we found that SpatialDropout2D\n    sometimes produced slightly better results for classification of ERP\n    signals. However, SpatialDropout2D significantly reduced performance\n    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n    the default Dropout in most cases.\n\n    Assumes the input signal is sampled at 128Hz. If you want to use this model\n    for any other sampling rate you will need to modify the lengths of temporal\n    kernels and average pooling size in blocks 1 and 2 as needed (double the\n    kernel lengths for double the sampling rate, etc). Note that we haven't\n    tested the model performance with this rule so this may not work well.\n\n    The model with default parameters gives the EEGNet-8,2 model as discussed\n    in the paper. This model should do pretty well in general, although it is\n\tadvised to do some model searching to get optimal performance on your\n\tparticular dataset.\n    We set F2 = F1 * D (number of input filters = number of output filters) for\n    the SeparableConv2D layer. We haven't extensively tested other values of this\n    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n    overcomplete). We believe the main parameters to focus on are F1 and D.\n    Inputs:\n\n      nb_classes      : int, number of classes to classify\n      Chans, Samples  : number of channels and time points in the EEG data\n      dropoutRate     : dropout fraction\n      kernLength      : length of temporal convolution in first layer. We found\n                        that setting this to be half the sampling rate worked\n                        well in practice. For the SMR dataset in particular\n                        since the data was high-passed at 4Hz we used a kernel\n                        length of 32.\n      F1, F2          : number of temporal filters (F1) and number of pointwise\n                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D.\n      D               : number of spatial filters to learn within each temporal\n                        convolution. Default: D = 2\n      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n    \"\"\"\n\n    if dropoutType == 'SpatialDropout2D':\n        dropoutType = SpatialDropout2D\n    elif dropoutType == 'Dropout':\n        dropoutType = Dropout\n    else:\n        raise ValueError('dropoutType must be one of SpatialDropout2D '\n                         'or Dropout, passed as a string.')\n\n    input1   = Input(shape = (Chans, Samples, 1))\n\n    ##################################################################\n    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n                                   name='Conv2D_1',\n                                   input_shape = (Chans, Samples, 1),\n                                   use_bias = False)(input1)\n    block1       = BatchNormalization()(block1)\n    block1       = DepthwiseConv2D((Chans, 1), use_bias = False,\n                                   name='Depth_wise_Conv2D_1',\n                                   depth_multiplier = D,\n                                   depthwise_constraint = max_norm(1.))(block1)\n    block1       = BatchNormalization()(block1)\n    block1       = Activation('elu')(block1)\n    block1       = AveragePooling2D((1, 4))(block1)\n    block1       = dropoutType(dropoutRate)(block1)\n\n    block2       = SeparableConv2D(F2, (1, 16),\n                                   name='Separable_Conv2D_1',\n                                   use_bias = False, padding = 'same')(block1)\n    block2       = BatchNormalization()(block2)\n    block2       = Activation('elu')(block2)\n    block2       = AveragePooling2D((1, 8))(block2)\n    block2       = dropoutType(dropoutRate)(block2)\n\n    flatten      = Flatten(name = 'flatten')(block2)\n\n    dense_label1        = Dense(nb_classes, name = 'output_valence', activation = \"sigmoid\",\n                         kernel_constraint = max_norm(norm_rate),  )(flatten)\n    dense_label2        = Dense(nb_classes, name = 'output_arousal', activation = \"sigmoid\",\n                         kernel_constraint = max_norm(norm_rate))(flatten)\n    #softmax      = Activation('softmax', name = 'out_activation')(dense)\n\n    return Model(inputs=input1, outputs=[dense_label1, dense_label2])","metadata":{"id":"Co7l7dJNFAT1","executionInfo":{"status":"error","timestamp":1697915777030,"user_tz":-180,"elapsed":768,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"}},"outputId":"cb7d7889-477f-4052-f91d-5f29a196011b","execution":{"iopub.status.busy":"2025-01-16T14:30:55.127433Z","iopub.execute_input":"2025-01-16T14:30:55.127748Z","iopub.status.idle":"2025-01-16T14:30:55.155297Z","shell.execute_reply.started":"2025-01-16T14:30:55.127723Z","shell.execute_reply":"2025-01-16T14:30:55.154003Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Training","metadata":{"id":"HhoVzuYszHuJ"}},{"cell_type":"code","source":"#general Info\nN_subs = 32\nN_trials = 40\nN_chann = 32\nfs = 128 #Hz\nlen_data = 8064\nlabels = 0 #(valence, arousal, dominance, liking)\nf_bank = np.asarray([4., 40.])\nn_splits = 5\n\nlen_wind = 6 # 6 segs windows\nN_windws =len_data/ (len_wind*fs) # 10 windows from 6 seconds\n\n\n\n#train_test_data\n\nXpr_train = Prolls_array_train[:,24:88,:,np.newaxis].astype('float32').transpose(0,2,1,3)\nypr_train = Prolls_train_mask[:,24:88,:,np.newaxis].astype('float32').transpose(0,2,1,3)\nXpr_dims = Xpr_train.shape\n\n\nprint('EEG variables names: \"X_train\"' )\nprint('MIDI variables names: \"Xpr_train\"')\n\nimg_size = Xpr_dims[1::]\nnum_classes = 1\nloss = DiceCoefficient()\n\n#wide net parameters\nExperiment = 'cross_val'\ndata_set = 'DEAP'\nnunits = 1000\ndroprate = 0.4\nl1 = 1e-4 #\nl2 = l1\nverbose = 0\n\n\n#load paths\nEEG_net_paths = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/EEG/cross_val_results/'\n\n\n#save paths\nsave_generated_midis = \"/content/drive/MyDrive/music_VAEs/DEAP_dataset/match_networks/match_cross_val/generated_database/\"\nsave_match_models_weigths = \"/content/drive/MyDrive/music_VAEs/DEAP_dataset/match_networks/match_cross_val/match_net_models/\"","metadata":{"id":"VFOhlzE8zMSs","executionInfo":{"status":"ok","timestamp":1694811747019,"user_tz":300,"elapsed":8,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"}},"outputId":"e0f36b0f-8f14-47b0-a593-162bc457606a","execution":{"iopub.status.busy":"2025-01-16T14:30:55.158928Z","iopub.execute_input":"2025-01-16T14:30:55.159348Z","iopub.status.idle":"2025-01-16T14:30:55.196773Z","shell.execute_reply.started":"2025-01-16T14:30:55.159313Z","shell.execute_reply":"2025-01-16T14:30:55.195253Z"},"trusted":true},"outputs":[{"name":"stdout","text":"EEG variables names: \"X_train\"\nMIDI variables names: \"Xpr_train\"\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"path_DEAP = '/kaggle/input/deap-dataset'\nsave_path = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/'\nsave_models_path = '/content/drive/MyDrive/music_VAEs/DEAP_dataset/EEG/cross_val_results/'\n\n\nN_subs = 32\nN_trials = 40\nN_chann = 32\nfs = 128 #Hz\nlen_data = 7680# len without the initial seconds 8064\nlen_wind = 6 # 6 segs windows\nN_windws =len_data/ (len_wind*fs) # 10 windows from 6 seconds\n\n\nlabels = 1 #(valence, arousal, dominance, liking)\nf_bank = np.asarray([4., 40.])\nlr = 1e-2\nepochs = 500\nverbose = 0\n\nseed = 123\nfolds = 5\n\n##save data\n\ndata_set = 'DEAP'\nmodel_name = 'multiclass'\nExperiment = 'cross_val'","metadata":{"id":"YwqeDZAqzOIo","execution":{"iopub.status.busy":"2025-01-16T14:30:55.199005Z","iopub.execute_input":"2025-01-16T14:30:55.199659Z","iopub.status.idle":"2025-01-16T14:30:55.209055Z","shell.execute_reply.started":"2025-01-16T14:30:55.199612Z","shell.execute_reply":"2025-01-16T14:30:55.207243Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"!mkdir /kaggle/working/all_windows_piano_roll_generated","metadata":{"execution":{"iopub.status.busy":"2025-01-16T14:30:55.211775Z","iopub.execute_input":"2025-01-16T14:30:55.217822Z","iopub.status.idle":"2025-01-16T14:30:56.429888Z","shell.execute_reply.started":"2025-01-16T14:30:55.217761Z","shell.execute_reply":"2025-01-16T14:30:56.42833Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from keras.utils import plot_model\n","metadata":{"execution":{"iopub.status.busy":"2025-01-16T14:30:56.43222Z","iopub.execute_input":"2025-01-16T14:30:56.43262Z","iopub.status.idle":"2025-01-16T14:30:56.438754Z","shell.execute_reply.started":"2025-01-16T14:30:56.432584Z","shell.execute_reply":"2025-01-16T14:30:56.437417Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for sub in range(32):\n\n    print('Subject: '+ str (sub+1))\n    #-------- load subject\n    with open('/kaggle/input/deap-full/DEAP/data_preprocessed_python/s'+ f\"{sub+1:02}\" + '.dat', 'rb') as file:\n        subject = pickle.load(file, encoding='latin1')\n    X = subject['data'][:,0:N_chann,int(3*fs):]\n\n    ### target data\n    y_valance = (subject['labels'][:,0] > 5)*1\n    y_arousal = (subject['labels'][:,1] > 5)*1\n\n    y = []\n    for i in range(len(y_valance)):\n      if y_valance[i] == 1 and y_arousal[i] == 1:\n        y.append(0)\n      elif y_valance[i] == 0 and y_arousal[i] == 1:\n        y.append(1)\n      elif y_valance[i] == 0 and y_arousal[i] == 0:\n        y.append(2)\n      elif y_valance[i] == 1 and y_arousal[i] == 0:\n        y.append(3)\n\n    y = np.array(y)\n\n\n    Xdata = segmentation_trials(X, fs = fs, segs = len_wind ) # windowing 1 min trials in 6 seconds segments\n    Xdata = Xdata[indx_full_audio,:,:,np.newaxis]\n\n    #All train losses\n    ###### multiclass\n    y_data_multiclass = np.repeat(y, N_windws)[indx_full_audio] # reapiting layers 10 times\n    ##### biclass\n    y_data_valence = np.repeat(y_valance, N_windws)[indx_full_audio]\n    y_data_arousal = np.repeat(y_arousal, N_windws)[indx_full_audio]\n\n    Y_train = [y_data_valence, y_data_arousal]\n\n\n    print(Xdata.shape, y_data_multiclass.shape)\n\n    # not one hot\n    y_valance = subject['labels'][:,0] / 10\n    y_arousal = subject['labels'][:,1] / 10\n\n    y_continuos_valence = np.repeat(y_valance, N_windws)[indx_full_audio]\n    y_continuos_arousal = np.repeat(y_arousal, N_windws)[indx_full_audio]\n\n    #--------- load Network\n\n    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=30, verbose=verbose, mode='min', min_delta=0.01, min_lr=0)\n    terminate_on_nan = tf.keras.callbacks.TerminateOnNaN()\n    callbacks = [reduce_lr_on_plateau, terminate_on_nan]\n    #losses = [tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryCrossentropy()]\n\n    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n    for i, (train_index, test_index) in enumerate(sss.split(Xdata, y_data_valence)):\n      print(f\"fold  N°{i+1}\")\n      Xtrain_ = Xdata[train_index]\n      Xtest_ = Xdata[test_index]\n\n      y_train_valence = y_data_valence[train_index]\n      y_test_valence = y_data_valence[test_index]\n\n      y_train_arousal = y_data_arousal[train_index]\n      y_test_arousal = y_data_arousal[test_index]\n\n\n      ytrain_ = y_data_multiclass[train_index]\n      ytest_ = y_data_multiclass[test_index]\n\n      ### load pesos autoencoder\n\n      img_size = Xpr_train.shape[-3::]\n      loss = [DiceCoefficient(), custom_loss(scaleX=1,scaleY=0.5)]\n      cka_pr = autoencoderCKA_Pianoroll(img_size, loss, Q2 = 2, validation_split=0.2, epochs=250, Dice_weigths = [0.7, 0.3])\n      cka_pr.get_model()\n\n      cka_pr.model.load_weights(f'/kaggle/input/midi-featuresandweigths/autoencoder_cka_weigths/ae_pr_sub{sub+1}.h5')\n\n      ### load EEGnet\n      losses = [tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanSquaredError()]\n\n      opt = tf.keras.optimizers.Adam(learning_rate= 0.01)\n      batch_size, Chans, Samples, _ = Xdata.shape\n      model_args = {'nb_classes':1,'Chans':Chans,'Samples':Samples,'dropoutRate':0.6,\n                  'kernLength':128,'F1':4,'D':4,'F2':32,'norm_rate':0.25,'dropoutType':'Dropout'}\n      clf = EEGNet_two_output(nb_classes = model_args['nb_classes'],\n                          Chans = model_args['Chans'],\n                          Samples = model_args['Samples'],\n                          dropoutRate = model_args['dropoutRate'],\n                          kernLength = 128,\n                          F1 = model_args['F1'], D = model_args['D'], F2 = model_args['F2'],\n                          )\n      clf.compile(loss= losses, optimizer=opt)\n      # history = clf.fit( Xtrain_,[y_train_valence[:,np.newaxis], y_train_arousal[:,np.newaxis]], epochs = 500,\n      #                   batch_size=500, callbacks=callbacks,\n      #                    validation_split=0.2, verbose=1\n      #                    )\n    \n      from keras.utils import plot_model\n      from IPython.display import Image\n\n      # Graficar el modelo del autoencoder CKA\n      plot_model(cka_pr.model, to_file='cka_pr_model.png', show_shapes=True, show_layer_names=True)\n      Image('cka_pr_model.png')\n\n      # Graficar el modelo EEGNet\n      plot_model(clf, to_file='eegnet_model.png', show_shapes=True, show_layer_names=True)\n      Image('eegnet_model.png')\n\n      history = clf.fit( Xtrain_,[y_continuos_valence[train_index,np.newaxis], y_continuos_arousal[train_index,np.newaxis]], epochs = 500,\n                        batch_size=500, callbacks=callbacks,\n                        validation_split=0.2, verbose=0\n                        )\n\n      y_pred_valence, y_pred_arousal=clf.predict(Xtest_)\n\n      print(accuracy_score(np.where(y_pred_valence > 0.5, 1, 0 ), y_test_valence))\n      print(accuracy_score(np.where(y_pred_arousal > 0.5, 1, 0 ), y_test_arousal))\n\n      # generation\n\n      y_pred_array = np.concatenate((y_pred_valence,y_pred_arousal), axis = 1)\n      y_true_array = np.concatenate((y_continuos_valence[train_index,np.newaxis],y_continuos_arousal[train_index,np.newaxis]), axis = 1)\n      # distance matrix\n\n      y_dist = cdist(y_pred_array, y_true_array, 'euclidean')\n    \n      \n      sigmoid_ydist   = tf.keras.layers.Softmax(axis=-1, name = 'softmax')(y_dist)\n      #dense_ydist   = Dense(y_dist.shape[-1], name = 'softmax', activation = \"softmax\")(y_dist)\n\n      # select top 5 nearest\n      max_indices = np.argsort(-sigmoid_ydist.numpy(), axis=1)[:, :5]\n      # Crear una matriz con 1 en los índices de los valores máximos y 0 en el resto\n      result_array = np.zeros_like(sigmoid_ydist.numpy())\n      rows = np.arange(sigmoid_ydist.shape[0])[:, np.newaxis]\n      result_array[rows, max_indices] = 1\n      #piano roll Generation\n      Xpr_code= cka_pr.encoder(Xpr_train[train_index])\n      print(sigmoid_ydist.shape, Xpr_code.shape)\n      eeg_coding = np.matmul(result_array, Xpr_code)\n      Xpr_pred = cka_pr.decoder(eeg_coding).numpy()\n      #saving arrays\n      np.save(f'/kaggle/working/all_windows_piano_roll_generated/sub{sub}_fold{i}', Xpr_pred)\n","metadata":{"id":"YOuvyNb_zSEZ","executionInfo":{"status":"ok","timestamp":1694815919382,"user_tz":300,"elapsed":57660,"user":{"displayName":"Hernan Dario Perez Nastar","userId":"17600740636788319470"}},"outputId":"f2ef895d-4a2c-4109-885a-5b305bd9c277","execution":{"iopub.status.busy":"2025-01-16T14:31:47.838685Z","iopub.execute_input":"2025-01-16T14:31:47.839109Z","iopub.status.idle":"2025-01-16T14:31:47.96828Z","shell.execute_reply.started":"2025-01-16T14:31:47.839063Z","shell.execute_reply":"2025-01-16T14:31:47.966566Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Subject: 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m (sub\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#-------- load subject\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/deap-full/DEAP/data_preprocessed_python/s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msub\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m02\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m     subject \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m0\u001b[39m:N_chann,\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mfs):]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/deap-full/DEAP/data_preprocessed_python/s01.dat'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/deap-full/DEAP/data_preprocessed_python/s01.dat'","output_type":"error"}],"execution_count":30},{"cell_type":"markdown","source":"# CONVERSION MIDI","metadata":{}},{"cell_type":"code","source":"def rebuilPianoRolls(X):\n  \"\"\"\n  this function takes an output array from the vae net and returns\n  a piano roll array ready to be converted into MIDI file\n  input\n  X: array with dimensions [samples, time, pitch, 1]\n  output\n  proll_padding: array with dimension [sample x pitch x time ]\n  \"\"\"\n  #trasnpose to [sample x time x pitch]\n  proll = X[:,:,:,0].transpose(0,2,1)\n  proll_padding =np.pad(proll, ((0,0), (24,40), (0, 0)), 'constant')\n  return proll_padding","metadata":{"id":"vZnw8BXgFsVR","execution":{"iopub.status.busy":"2025-01-16T14:30:56.733588Z","iopub.status.idle":"2025-01-16T14:30:56.733976Z","shell.execute_reply.started":"2025-01-16T14:30:56.733796Z","shell.execute_reply":"2025-01-16T14:30:56.733813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/all_windows_MIDI_generated","metadata":{"execution":{"iopub.status.busy":"2025-01-16T14:30:56.735856Z","iopub.status.idle":"2025-01-16T14:30:56.736419Z","shell.execute_reply.started":"2025-01-16T14:30:56.736145Z","shell.execute_reply":"2025-01-16T14:30:56.736173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_generated_midis = f\"/kaggle/working/all_windows_MIDI_generated\"\n\nfor sub in range(32):\n\n  print(f'saving MIDI sub: {sub}')\n  for fold in range(5):\n    midiGEnerated = np.load(f'/kaggle/working/all_windows_piano_roll_generated/sub{sub}_fold{fold}.npy')\n    midiGEnerated = rebuilPianoRolls(midiGEnerated)\n    print(f'{\"saving MIDI generated\":-^50}')\n     # --- create directoris to save MIDI ------\n    midi_save_path = os.path.join(save_generated_midis, 'MIDI_sub_'+str(sub+1))\n    try:\n        os.mkdir(midi_save_path)\n    except OSError as error:\n        print(error)\n    #----- saving data --------\n    for sample in range(midiGEnerated.shape[0]):\n      proll = np.where((midiGEnerated[sample])>0.1,100,0)\n      mid_new = piano_roll_to_pretty_midi(proll, fs=fProll)\n      mid_new.write(midi_save_path+'/mid_fold'+str(fold)+'_sub_'+str(sample)+'.mid')\n    print('--------- MIDI data saved---')\n    print(f'{\"MIDI data saved\":-^50}')\n\n","metadata":{"id":"1BaaHBEBtEpk","execution":{"iopub.status.busy":"2025-01-16T14:30:56.738067Z","iopub.status.idle":"2025-01-16T14:30:56.738655Z","shell.execute_reply.started":"2025-01-16T14:30:56.738386Z","shell.execute_reply":"2025-01-16T14:30:56.738413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"midiGEnerated = np.load(f'/kaggle/working/all_windows_piano_roll_generated/sub{11}_fold{4}.npy')\n\nfor sample in range(10):\n\n  # Datos de predicción\n  result_array_prediccion = midiGEnerated[sample,:,:,0]\n\n  result_array_prediccion2 = midiGEnerated[sample*2,:,:,0]\n\n  plt.figure(figsize=(9, 2))\n  # Primer subplot para los datos originales\n  plt.subplot(1, 2, 1)\n  plt.pcolormesh(np.where(result_array_prediccion.T > 0.01, 1, 0))\n  plt.colorbar()\n  plt.title(\"Original\")\n  # Segundo subplot para los datos de predicción\n  plt.subplot(1, 2, 2)\n  plt.pcolormesh(np.where(result_array_prediccion2.T > 0.01, 1, 0))\n  plt.colorbar()\n  plt.title(\"Prediction\")\n  # Ajustar el espaciado entre subplots\n  plt.tight_layout()\n  # Mostrar la figura con los dos subplots\n  plt.show()","metadata":{"id":"vP1v9lqwM-Eo","execution":{"iopub.status.busy":"2025-01-16T14:30:56.741035Z","iopub.status.idle":"2025-01-16T14:30:56.741482Z","shell.execute_reply.started":"2025-01-16T14:30:56.741299Z","shell.execute_reply":"2025-01-16T14:30:56.741317Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Y3MM-qzSFsn1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"AxRk-51wFsqn","trusted":true},"outputs":[],"execution_count":null}]}